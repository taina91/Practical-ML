{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ ‚Ññ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tainazitina/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_selection import chi2\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_data = pd.read_csv('human_text.txt', sep='\\t', header=None, names=['text'])\n",
    "robot_data = pd.read_csv('robot_text.txt', sep='\\t', header=None, names=['text'])\n",
    "\n",
    "# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫ –∫–ª–∞—Å—Å–æ–≤\n",
    "human_data['label'] = 0  # 0 - —á–µ–ª–æ–≤–µ–∫\n",
    "robot_data['label'] = 1  # 1 - —Ä–æ–±–æ—Ç\n",
    "\n",
    "# –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö\n",
    "data = pd.concat([human_data, robot_data], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tainazitina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# –≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ\n",
    "data['lowercase'] = data['text'].apply(lambda x: x.lower())\n",
    "\n",
    "def stop_words(text):\n",
    "    words = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# –±–µ–∑ —Å—Ç–æ–ø —Å–ª–æ–≤\n",
    "data['stop_words'] = data['text'].apply(lambda x: stop_words(x))\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "# X = vectorizer.fit_transform(data['text'])\n",
    "# y = data['label']\n",
    "\n",
    "# chi2_stat, p_values = chi2(X, y)\n",
    "# selected_words_indices = np.argsort(chi2_stat)[-100:]\n",
    "# selected_words = [vectorizer.get_feature_names_out()[i] for i in selected_words_indices]\n",
    "\n",
    "# filtered_data = []\n",
    "# for text in data['text']:\n",
    "#   words = text.split()\n",
    "#   filtered_words = [word for word in words if word in selected_words]\n",
    "#   filtered_data.append(' '.join(filtered_words))\n",
    "# # —Ç–æ–ª—å–∫–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–ª–æ–≤–∞\n",
    "# data['filter'] = filtered_data\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>human</th>\n",
       "      <th>robot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>friedrich</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>why</td>\n",
       "      <td>0.002416</td>\n",
       "      <td>0.001805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ago..</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>estaba</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93%</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6691</th>\n",
       "      <td>dice,</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6692</th>\n",
       "      <td>design,</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6693</th>\n",
       "      <td>gentlemen.</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6694</th>\n",
       "      <td>uuuuuuh</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6695</th>\n",
       "      <td>(2002).</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6696 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            text     human     robot\n",
       "0      friedrich  0.000000  0.000043\n",
       "1            why  0.002416  0.001805\n",
       "2          ago..  0.000000  0.000043\n",
       "3         estaba  0.000045  0.000043\n",
       "4            93%  0.000000  0.000043\n",
       "...          ...       ...       ...\n",
       "6691       dice,  0.000045  0.000000\n",
       "6692     design,  0.000045  0.000000\n",
       "6693  gentlemen.  0.000045  0.000000\n",
       "6694     uuuuuuh  0.000045  0.000000\n",
       "6695     (2002).  0.000000  0.000043\n",
       "\n",
       "[6696 rows x 3 columns]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "human_combined_string = ' '.join(human_data['text'])\n",
    "human_words = human_combined_string.split()\n",
    "human_word_frequency = Counter(human_words)\n",
    "human_frequency_dict = {key: value / len(human_words) for key, value in human_word_frequency.items()}\n",
    "\n",
    "\n",
    "robot_combined_string = ' '.join(robot_data['text'])\n",
    "robot_words = robot_combined_string.split()\n",
    "robot_word_frequency = Counter(robot_words)\n",
    "robot_frequency_dict = {key: value / len(robot_words) for key, value in robot_word_frequency.items()}\n",
    "  \n",
    "unique_words = set(human_words + robot_words)\n",
    "\n",
    "result_array = []\n",
    "for word in unique_words:\n",
    "  freq1 = human_frequency_dict.get(word, 0)\n",
    "  freq2 = robot_frequency_dict.get(word, 0)\n",
    "  result_array.append([word, freq1, freq2])\n",
    "\n",
    "unic_human_word = []\n",
    "unic_robot_word = []\n",
    "\n",
    "for i in range(len(result_array)):\n",
    "  if result_array[i][1] == 0:\n",
    "    unic_robot_word.append(result_array[i][0])\n",
    "  if result_array[i][2] == 0:\n",
    "    unic_human_word.append(result_array[i][0])\n",
    "  \n",
    "filtered_data = []\n",
    "for text in data['text']:\n",
    "  words = text.split()\n",
    "  filtered_words = [word for word in words if word in unic_human_word or word in unic_robot_word]\n",
    "  filtered_data.append(' '.join(filtered_words))\n",
    "# —Ç–æ–ª—å–∫–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–ª–æ–≤–∞\n",
    "data['filter'] = filtered_data\n",
    "\n",
    "df = pd.DataFrame(result_array, columns = ['text', 'human', 'robot'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>lowercase</th>\n",
       "      <th>stop_words</th>\n",
       "      <th>filter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4597</th>\n",
       "      <td>si ?</td>\n",
       "      <td>1</td>\n",
       "      <td>si ?</td>\n",
       "      <td>si ?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4598</th>\n",
       "      <td>no tengo el placer de conocer ese lugar !  tu ...</td>\n",
       "      <td>1</td>\n",
       "      <td>no tengo el placer de conocer ese lugar !  tu ...</td>\n",
       "      <td>tengo el placer de conocer ese lugar ! tu que ...</td>\n",
       "      <td>placer conocer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4599</th>\n",
       "      <td>me gustar√≠a conocerlo entonces !  !  üòÅ</td>\n",
       "      <td>1</td>\n",
       "      <td>me gustar√≠a conocerlo entonces !  !  üòÅ</td>\n",
       "      <td>gustar√≠a conocerlo entonces ! ! üòÅ</td>\n",
       "      <td>conocerlo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4600</th>\n",
       "      <td>ooooh !</td>\n",
       "      <td>1</td>\n",
       "      <td>ooooh !</td>\n",
       "      <td>ooooh !</td>\n",
       "      <td>ooooh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4601</th>\n",
       "      <td>iiiiiiiiiiih !</td>\n",
       "      <td>1</td>\n",
       "      <td>iiiiiiiiiiih !</td>\n",
       "      <td>iiiiiiiiiiih !</td>\n",
       "      <td>iiiiiiiiiiih</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4692</th>\n",
       "      <td>hi here !  how are you ?</td>\n",
       "      <td>1</td>\n",
       "      <td>hi here !  how are you ?</td>\n",
       "      <td>hi ! ?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4693</th>\n",
       "      <td>nice !  i'm fine too</td>\n",
       "      <td>1</td>\n",
       "      <td>nice !  i'm fine too</td>\n",
       "      <td>nice ! i'm fine</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4694</th>\n",
       "      <td>what means m ?</td>\n",
       "      <td>1</td>\n",
       "      <td>what means m ?</td>\n",
       "      <td>means ?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4695</th>\n",
       "      <td>hi there !  !  how are you ?  üòÑ</td>\n",
       "      <td>1</td>\n",
       "      <td>hi there !  !  how are you ?  üòÑ</td>\n",
       "      <td>hi ! ! ? üòÑ</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4696</th>\n",
       "      <td>hi !</td>\n",
       "      <td>1</td>\n",
       "      <td>hi !</td>\n",
       "      <td>hi !</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label  \\\n",
       "4597                                              si ?       1   \n",
       "4598  no tengo el placer de conocer ese lugar !  tu ...      1   \n",
       "4599             me gustar√≠a conocerlo entonces !  !  üòÅ      1   \n",
       "4600                                           ooooh !       1   \n",
       "4601                                    iiiiiiiiiiih !       1   \n",
       "...                                                 ...    ...   \n",
       "4692                          hi here !  how are you ?       1   \n",
       "4693                               nice !  i'm fine too      1   \n",
       "4694                                    what means m ?       1   \n",
       "4695                    hi there !  !  how are you ?  üòÑ      1   \n",
       "4696                                              hi !       1   \n",
       "\n",
       "                                              lowercase  \\\n",
       "4597                                              si ?    \n",
       "4598  no tengo el placer de conocer ese lugar !  tu ...   \n",
       "4599             me gustar√≠a conocerlo entonces !  !  üòÅ   \n",
       "4600                                           ooooh !    \n",
       "4601                                    iiiiiiiiiiih !    \n",
       "...                                                 ...   \n",
       "4692                          hi here !  how are you ?    \n",
       "4693                               nice !  i'm fine too   \n",
       "4694                                    what means m ?    \n",
       "4695                    hi there !  !  how are you ?  üòÑ   \n",
       "4696                                              hi !    \n",
       "\n",
       "                                             stop_words          filter  \n",
       "4597                                               si ?                  \n",
       "4598  tengo el placer de conocer ese lugar ! tu que ...  placer conocer  \n",
       "4599                  gustar√≠a conocerlo entonces ! ! üòÅ       conocerlo  \n",
       "4600                                            ooooh !           ooooh  \n",
       "4601                                     iiiiiiiiiiih !    iiiiiiiiiiih  \n",
       "...                                                 ...             ...  \n",
       "4692                                             hi ! ?                  \n",
       "4693                                    nice ! i'm fine                  \n",
       "4694                                            means ?                  \n",
       "4695                                         hi ! ! ? üòÑ                  \n",
       "4696                                               hi !                  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.2, random_state=42)\n",
    "X_train_lower, X_test_lower, y_train_lower, y_test_lower = train_test_split(data['lowercase'], data['label'], test_size=0.2, random_state=42)\n",
    "X_train_stop, X_test_stop, y_train_stop, y_test_stop = train_test_split(data['stop_words'], data['label'], test_size=0.2, random_state=42)\n",
    "X_train_filter, X_test_filter, y_train_filter, y_test_filter = train_test_split(data['filter'], data['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "X_train_lower_vectorized = vectorizer.fit_transform(X_train_lower)\n",
    "X_test_lower_vectorized = vectorizer.transform(X_test_lower)\n",
    "\n",
    "X_train_stop_vectorized = vectorizer.fit_transform(X_train_stop)\n",
    "X_test_stop_vectorized = vectorizer.transform(X_test_stop)\n",
    "\n",
    "X_train_filter_vectorized = vectorizer.fit_transform(X_train_filter)\n",
    "X_test_filter_vectorized = vectorizer.transform(X_test_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB() Accuracy original text: 0.7160\n",
      "MultinomialNB() Accuracy lower text: 0.7160\n",
      "MultinomialNB() Accuracy without stopwords: 0.6915\n",
      "MultinomialNB() Accuracy without filter words: 0.6936\n"
     ]
    }
   ],
   "source": [
    "NaiveBayes = MultinomialNB()\n",
    "\n",
    "NaiveBayes.fit(X_train_vectorized, y_train)\n",
    "y_pred = NaiveBayes.predict(X_test_vectorized)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'{NaiveBayes} Accuracy original text: {accuracy:.4f}')\n",
    "\n",
    "NaiveBayes.fit(X_train_lower_vectorized, y_train_lower)\n",
    "y_pred = NaiveBayes.predict(X_test_lower_vectorized)\n",
    "accuracy = accuracy_score(y_test_lower, y_pred)\n",
    "print(f'{NaiveBayes} Accuracy lower text: {accuracy:.4f}')\n",
    "\n",
    "NaiveBayes.fit(X_train_stop_vectorized, y_train_stop)\n",
    "y_pred = NaiveBayes.predict(X_test_stop_vectorized)\n",
    "accuracy = accuracy_score(y_test_stop, y_pred)\n",
    "print(f'{NaiveBayes} Accuracy without stopwords: {accuracy:.4f}')\n",
    "\n",
    "NaiveBayes.fit(X_train_filter_vectorized, y_train_filter)\n",
    "y_pred = NaiveBayes.predict(X_test_filter_vectorized)\n",
    "accuracy = accuracy_score(y_test_filter, y_pred)\n",
    "print(f'{NaiveBayes} Accuracy without filter words: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC() Accuracy original text: 0.7330\n",
      "SVC() Accuracy lower text: 0.7330\n",
      "SVC() Accuracy without stopwords: 0.6883\n",
      "SVC() Accuracy without filter words: 0.6798\n"
     ]
    }
   ],
   "source": [
    "SVM = SVC()\n",
    "\n",
    "SVM.fit(X_train_vectorized, y_train)\n",
    "y_pred = SVM.predict(X_test_vectorized)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'{SVM} Accuracy original text: {accuracy:.4f}')\n",
    "\n",
    "SVM.fit(X_train_lower_vectorized, y_train_lower)\n",
    "y_pred = SVM.predict(X_test_lower_vectorized)\n",
    "accuracy = accuracy_score(y_test_lower, y_pred)\n",
    "print(f'{SVM} Accuracy lower text: {accuracy:.4f}')\n",
    "\n",
    "SVM.fit(X_train_stop_vectorized, y_train_stop)\n",
    "y_pred = SVM.predict(X_test_stop_vectorized)\n",
    "accuracy = accuracy_score(y_test_stop, y_pred)\n",
    "print(f'{SVM} Accuracy without stopwords: {accuracy:.4f}')\n",
    "\n",
    "SVM.fit(X_train_filter_vectorized, y_train_filter)\n",
    "y_pred = SVM.predict(X_test_filter_vectorized)\n",
    "accuracy = accuracy_score(y_test_filter, y_pred)\n",
    "print(f'{SVM} Accuracy without filter words: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier() Accuracy original text: 0.6021\n",
      "KNeighborsClassifier() Accuracy lower text: 0.6021\n",
      "KNeighborsClassifier() Accuracy without stopwords: 0.5862\n",
      "KNeighborsClassifier() Accuracy without filter words: 0.5809\n"
     ]
    }
   ],
   "source": [
    "kNN = KNeighborsClassifier()\n",
    "\n",
    "kNN.fit(X_train_vectorized, y_train)\n",
    "y_pred = kNN.predict(X_test_vectorized)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'{kNN} Accuracy original text: {accuracy:.4f}')\n",
    "\n",
    "kNN.fit(X_train_lower_vectorized, y_train_lower)\n",
    "y_pred = kNN.predict(X_test_lower_vectorized)\n",
    "accuracy = accuracy_score(y_test_lower, y_pred)\n",
    "print(f'{kNN} Accuracy lower text: {accuracy:.4f}')\n",
    "\n",
    "kNN.fit(X_train_stop_vectorized, y_train_stop)\n",
    "y_pred = kNN.predict(X_test_stop_vectorized)\n",
    "accuracy = accuracy_score(y_test_stop, y_pred)\n",
    "print(f'{kNN} Accuracy without stopwords: {accuracy:.4f}')\n",
    "\n",
    "kNN.fit(X_train_filter_vectorized, y_train_filter)\n",
    "y_pred = kNN.predict(X_test_filter_vectorized)\n",
    "accuracy = accuracy_score(y_test_filter, y_pred)\n",
    "print(f'{kNN} Accuracy without filter words: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier() Accuracy original text: 0.7309\n",
      "Top 10 important features for RandomForestClassifier():\n",
      "  Feature    Importance\n",
      "0      00  4.119896e-05\n",
      "1     000  2.932514e-07\n",
      "2  00ffff  4.523483e-05\n",
      "3      01  1.035812e-05\n",
      "4      02  0.000000e+00\n",
      "5      03  3.715093e-06\n",
      "6      04  1.175418e-05\n",
      "7      05  0.000000e+00\n",
      "8      07  0.000000e+00\n",
      "9      10  5.310937e-05\n",
      "RandomForestClassifier() Accuracy lower text: 0.7309\n",
      "Top 10 important features for RandomForestClassifier():\n",
      "  Feature    Importance\n",
      "0      00  1.250705e-04\n",
      "1     000  6.616313e-07\n",
      "2  00ffff  1.805533e-05\n",
      "3      01  1.567021e-06\n",
      "4      02  0.000000e+00\n",
      "5      03  7.317620e-06\n",
      "6      04  2.682095e-06\n",
      "7      05  0.000000e+00\n",
      "8      07  0.000000e+00\n",
      "9      10  3.643545e-05\n",
      "RandomForestClassifier() Accuracy without stopwords: 0.6670\n",
      "Top 10 important features for RandomForestClassifier():\n",
      "  Feature  Importance\n",
      "0      00    0.000052\n",
      "1     000    0.000004\n",
      "2  00ffff    0.000100\n",
      "3      01    0.000007\n",
      "4      02    0.000000\n",
      "5      03    0.000026\n",
      "6      04    0.000116\n",
      "7      05    0.000000\n",
      "8      07    0.000000\n",
      "9      10    0.000037\n",
      "RandomForestClassifier() Accuracy without filter words: 0.6596\n",
      "Top 10 important features for RandomForestClassifier():\n",
      "  Feature  Importance\n",
      "0      00    0.000510\n",
      "1     000    0.000000\n",
      "2  00ffff    0.000315\n",
      "3      01    0.000000\n",
      "4      02    0.000000\n",
      "5      03    0.000390\n",
      "6      04    0.000312\n",
      "7      05    0.000005\n",
      "8      07    0.000000\n",
      "9      10    0.000109\n"
     ]
    }
   ],
   "source": [
    "RandomForest = RandomForestClassifier()\n",
    "\n",
    "RandomForest.fit(X_train_vectorized, y_train)\n",
    "y_pred = RandomForest.predict(X_test_vectorized)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'{RandomForest} Accuracy original text: {accuracy:.4f}')\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_importance = RandomForest.feature_importances_\n",
    "important_features = pd.DataFrame(list(zip(feature_names, feature_importance)), columns=['Feature', 'Importance'])\n",
    "print(f'Top 10 important features for {RandomForest}:\\n{important_features.head(10)}')\n",
    "\n",
    "RandomForest.fit(X_train_lower_vectorized, y_train_lower)\n",
    "y_pred = RandomForest.predict(X_test_lower_vectorized)\n",
    "accuracy = accuracy_score(y_test_lower, y_pred)\n",
    "print(f'{RandomForest} Accuracy lower text: {accuracy:.4f}')\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_importance = RandomForest.feature_importances_\n",
    "important_features = pd.DataFrame(list(zip(feature_names, feature_importance)), columns=['Feature', 'Importance'])\n",
    "print(f'Top 10 important features for {RandomForest}:\\n{important_features.head(10)}')\n",
    "\n",
    "RandomForest.fit(X_train_stop_vectorized, y_train_stop)\n",
    "y_pred = RandomForest.predict(X_test_stop_vectorized)\n",
    "accuracy = accuracy_score(y_test_stop, y_pred)\n",
    "print(f'{RandomForest} Accuracy without stopwords: {accuracy:.4f}')\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_importance = RandomForest.feature_importances_\n",
    "important_features = pd.DataFrame(list(zip(feature_names, feature_importance)), columns=['Feature', 'Importance'])\n",
    "print(f'Top 10 important features for {RandomForest}:\\n{important_features.head(10)}')\n",
    "\n",
    "RandomForest.fit(X_train_filter_vectorized, y_train_filter)\n",
    "y_pred = RandomForest.predict(X_test_filter_vectorized)\n",
    "accuracy = accuracy_score(y_test_filter, y_pred)\n",
    "print(f'{RandomForest} Accuracy without filter words: {accuracy:.4f}')\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "feature_importance = RandomForest.feature_importances_\n",
    "important_features = pd.DataFrame(list(zip(feature_names, feature_importance)), columns=['Feature', 'Importance'])\n",
    "print(f'Top 10 important features for {RandomForest}:\\n{important_features.head(10)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression() Accuracy original text: 0.7085\n",
      "Top 10 important features for LogisticRegression():\n",
      "             Feature  Importance\n",
      "1974           mejor    3.516494\n",
      "1184        extremas    3.230081\n",
      "2715          seemed    3.155412\n",
      "985          driving    2.616612\n",
      "2088           named    2.348790\n",
      "1408           grupo    2.253971\n",
      "2650  rottentomatoes    2.200000\n",
      "2101  necesariamente    2.195582\n",
      "1874           llama    2.003890\n",
      "1868        listened    1.895497\n",
      "LogisticRegression() Accuracy lower text: 0.7085\n",
      "Top 10 important features for LogisticRegression():\n",
      "             Feature  Importance\n",
      "1974           mejor    3.516494\n",
      "1184        extremas    3.230081\n",
      "2715          seemed    3.155412\n",
      "985          driving    2.616612\n",
      "2088           named    2.348790\n",
      "1408           grupo    2.253971\n",
      "2650  rottentomatoes    2.200000\n",
      "2101  necesariamente    2.195582\n",
      "1874           llama    2.003890\n",
      "1868        listened    1.895497\n",
      "LogisticRegression() Accuracy without stopwords: 0.6989\n",
      "Top 10 important features for LogisticRegression():\n",
      "          Feature  Importance\n",
      "1946        march    3.749192\n",
      "2677  santaolalla    3.500127\n",
      "971         donde    2.611910\n",
      "1172     explores    2.206745\n",
      "2614   respuestas    2.169063\n",
      "1388        gplv3    2.106130\n",
      "1806       lapang    1.959712\n",
      "3502           ŸÖŸÜ    1.862695\n",
      "1779      knowing    1.800988\n",
      "2980       talent    1.746220\n",
      "LogisticRegression() Accuracy without filter words: 0.6894\n",
      "Top 10 important features for LogisticRegression():\n",
      "       Feature  Importance\n",
      "1595        im    3.478891\n",
      "2174      okay    3.168038\n",
      "973       dont    2.158386\n",
      "3277      what    1.929805\n",
      "1457     hahha    1.753896\n",
      "1442  hahahaha    1.727050\n",
      "3371  yourself    1.668958\n",
      "3367       you    1.616080\n",
      "40          22    1.607374\n",
      "2534     rdany    1.554665\n"
     ]
    }
   ],
   "source": [
    "LogisticRegression = LogisticRegression()\n",
    "\n",
    "LogisticRegression.fit(X_train_vectorized, y_train)\n",
    "y_pred = LogisticRegression.predict(X_test_vectorized)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'{LogisticRegression} Accuracy original text: {accuracy:.4f}')\n",
    "\n",
    "important_features = important_features.sort_values(by='Importance', ascending=False)\n",
    "feature_importance = np.abs(LogisticRegression.coef_[0])\n",
    "important_features = pd.DataFrame(list(zip(feature_names, feature_importance)), columns=['Feature', 'Importance'])\n",
    "important_features = important_features.sort_values(by='Importance', ascending=False)\n",
    "print(f'Top 10 important features for {LogisticRegression}:\\n{important_features.head(10)}')\n",
    "\n",
    "LogisticRegression.fit(X_train_lower_vectorized, y_train_lower)\n",
    "y_pred = LogisticRegression.predict(X_test_lower_vectorized)\n",
    "accuracy = accuracy_score(y_test_lower, y_pred)\n",
    "print(f'{LogisticRegression} Accuracy lower text: {accuracy:.4f}')\n",
    "\n",
    "important_features = important_features.sort_values(by='Importance', ascending=False)\n",
    "feature_importance = np.abs(LogisticRegression.coef_[0])\n",
    "important_features = pd.DataFrame(list(zip(feature_names, feature_importance)), columns=['Feature', 'Importance'])\n",
    "important_features = important_features.sort_values(by='Importance', ascending=False)\n",
    "print(f'Top 10 important features for {LogisticRegression}:\\n{important_features.head(10)}')\n",
    "\n",
    "LogisticRegression.fit(X_train_stop_vectorized, y_train_stop)\n",
    "y_pred = LogisticRegression.predict(X_test_stop_vectorized)\n",
    "accuracy = accuracy_score(y_test_stop, y_pred)\n",
    "print(f'{LogisticRegression} Accuracy without stopwords: {accuracy:.4f}')\n",
    "\n",
    "important_features = important_features.sort_values(by='Importance', ascending=False)\n",
    "feature_importance = np.abs(LogisticRegression.coef_[0])\n",
    "important_features = pd.DataFrame(list(zip(feature_names, feature_importance)), columns=['Feature', 'Importance'])\n",
    "important_features = important_features.sort_values(by='Importance', ascending=False)\n",
    "print(f'Top 10 important features for {LogisticRegression}:\\n{important_features.head(10)}')\n",
    "\n",
    "LogisticRegression.fit(X_train_filter_vectorized, y_train_filter)\n",
    "y_pred = LogisticRegression.predict(X_test_filter_vectorized)\n",
    "accuracy = accuracy_score(y_test_filter, y_pred)\n",
    "print(f'{LogisticRegression} Accuracy without filter words: {accuracy:.4f}')\n",
    "\n",
    "important_features = important_features.sort_values(by='Importance', ascending=False)\n",
    "feature_importance = np.abs(LogisticRegression.coef_[0])\n",
    "important_features = pd.DataFrame(list(zip(feature_names, feature_importance)), columns=['Feature', 'Importance'])\n",
    "important_features = important_features.sort_values(by='Importance', ascending=False)\n",
    "print(f'Top 10 important features for {LogisticRegression}:\\n{important_features.head(10)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
